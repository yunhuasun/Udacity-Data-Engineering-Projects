{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Udacity Data Engineer Nanodegree - Capstone Project\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "* The project is to build an ETL pipeline that extracts data from I94 Immigration Data set and World Temperature Data set, processes them using Spark, and loads data into data lake as a set of dimensional tables. The I94 Immigration Data comes from the US National Tourism and Trade Office Source. The World Temperature Dat dataset comes from Kaggle Source. \n",
    "\n",
    "\n",
    "* The I94 Immigration Data data lake will allow analysts to find insightes in how many, and what type of visitors are coming to the US. \n",
    "\n",
    "* The World Temperature Data lake will allow anlaysts to compare the temporary change month by month, or year by year acrooss multiple major cities. \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "import datetime\n",
    "from  pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "\n",
    "##### I94 Immigration Data (Data Source is in SAS format) \n",
    "* This data comes from the US National Tourism and Trade Office. Data contains international visitor arrival statistics by world regions and select countries (including top 20), type of visa, mode of transportation, age groups, states visited, and the top ports of entry. \n",
    "* The data set contains 3,096,313 Rows\n",
    "\n",
    "##### World Temperature Data (Data sour is in CSV format) \n",
    "* This dataset is from Kaggle and contains monthly average temperature data at different country in the world wide.\n",
    "* https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data\n",
    "* The data set contains 8,599,212 Rows\n",
    "\n",
    "#### Tools\n",
    "* Python\n",
    "* Pandas - exploratory data analysis on small data set\n",
    "* PySpark - data processing on large data set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "1. Use pandas for exploratory data analysis to get an overview on these data sets\n",
    "2. Split data sets to dimensional tables and change column names for better understanding\n",
    "3. Utilize PySpark on one of the SAS data sets to test ETL data pipeline logic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1. Explore I94 Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  \\\n",
       "0     2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0   \n",
       "1     2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0   \n",
       "2      589494  1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0   \n",
       "3     2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0   \n",
       "4     3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0   \n",
       "\n",
       "   i94mode i94addr    ...     entdepu  matflag  biryear   dtaddto  gender  \\\n",
       "0      1.0      HI    ...         NaN        M   1955.0  07202016       F   \n",
       "1      1.0      TX    ...         NaN        M   1990.0  10222016       M   \n",
       "2      1.0      FL    ...         NaN        M   1940.0  07052016       M   \n",
       "3      1.0      CA    ...         NaN        M   1991.0  10272016       M   \n",
       "4      3.0      NY    ...         NaN        M   1997.0  07042016       F   \n",
       "\n",
       "  insnum airline        admnum  fltno  visatype  \n",
       "0    NaN      JL  5.658267e+10  00782        WT  \n",
       "1    NaN     *GA  9.436200e+10  XBLNG        B2  \n",
       "2    NaN      LH  5.578047e+10  00464        WT  \n",
       "3    NaN      QR  9.478970e+10  00739        B2  \n",
       "4    NaN     NaN  4.232257e+10   LAND        WT  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data here\n",
    "df_immi = pd.read_csv(\"immigration_data_sample.csv\")\n",
    "df_immi.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALC</th>\n",
       "      <th>ALCAN</th>\n",
       "      <th>AK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANC</td>\n",
       "      <td>ANCHORAGE</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BAR</td>\n",
       "      <td>BAKER AAF - BAKER ISLAND</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DAC</td>\n",
       "      <td>DALTONS CACHE</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PIZ</td>\n",
       "      <td>DEW STATION PT LAY DEW</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DTH</td>\n",
       "      <td>DUTCH HARBOR</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ALC                     ALCAN  AK\n",
       "0  ANC                 ANCHORAGE  AK\n",
       "1  BAR  BAKER AAF - BAKER ISLAND  AK\n",
       "2  DAC             DALTONS CACHE  AK\n",
       "3  PIZ    DEW STATION PT LAY DEW  AK\n",
       "4  DTH              DUTCH HARBOR  AK"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_City = pd.read_csv(\"Port_City.csv\")\n",
    "df_City.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2. Explore temperature data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Ã…rhus   \n",
       "1  1743-12-01                 NaN                            NaN  Ã…rhus   \n",
       "2  1744-01-01                 NaN                            NaN  Ã…rhus   \n",
       "3  1744-02-01                 NaN                            NaN  Ã…rhus   \n",
       "4  1744-03-01                 NaN                            NaN  Ã…rhus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data here\n",
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temp = pd.read_csv(fname)\n",
    "df_temp.head(5)\n",
    "#df_temp.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data\n",
    "# Performing cleaning tasks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Data Cleaning is combined in the modeling steps. Below are the cleaniing steps that will be performed. \n",
    "* SAS format need to transfer to standard date format.\n",
    "* Since the Port City is Upper Case in the source table, we are going to convert the City to Uppder case in the dimentional table and Fact Table\n",
    "* Use Distinct in sql to remove duplicate in the dimension table.\n",
    "* Remove Null from the temperature data soure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Connect to Data Source\n",
    "\n",
    "#Read SAS file into df_immigration\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()\n",
    "df_immigration = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "\n",
    "#Create the view to run SQL quries\n",
    "df_immigration.createOrReplaceTempView(\"df_spark\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Connect to Data Source\n",
    "#Read CSV file into  df_temp\n",
    "df_temp=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv( '../../data2/GlobalLandTemperaturesByCity.csv')\n",
    "#Create the view to run SQL quries\n",
    "df_temp.createOrReplaceTempView(\"df_temp_tb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Connect to Data Source\n",
    "#Read CSV file into df_City\n",
    "Dim_PortCity = StructType([\n",
    "    StructField(\"PortCode\",StringType(),True),\n",
    "    StructField(\"City\",StringType(),True),\n",
    "    StructField(\"States\",StringType(),True),\n",
    "])\n",
    "df_City=spark.read.format(\"csv\").schema(Dim_PortCity).csv(\"Port_City.csv\")\n",
    "df_City.createOrReplaceTempView(\"df_City\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since the purpose of this data warehouse is for OLAP and BI app usage, we will model these data sets with star schema data modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Data Model.jpg in the project\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Define the data model \n",
    "\n",
    "Fact_immigration_Schema = StructType([\n",
    "    StructField(\"CICID\",FloatType(),True),\n",
    "    StructField(\"ArrivalDate\",DateType(),True),\n",
    "    StructField(\"I94DepartureDate\",DateType(),True),\n",
    "    StructField(\"i94modeID\",StringType(),True),\n",
    "    StructField(\"ArrivalFlag\",StringType(),True),\n",
    "    StructField(\"DepartureFlag\",StringType(),True),\n",
    "    StructField(\"UpdateFlag\",StringType(),True),\n",
    "    StructField(\"I94ArrivalMode\",StringType(),True),\n",
    "    StructField(\"I94Port\",StringType(),True),\n",
    "    StructField(\"I94Month\",StringType(),True),\n",
    "    StructField(\"I94Year\",StringType(),True)])\n",
    "\n",
    "Dim_Admission_Schema = StructType([\n",
    "    StructField(\"CICID\",FloatType(),True),\n",
    "    StructField(\"AddmissionNumber\",FloatType(),True),\n",
    "    StructField(\"BirthYear\",FloatType(),True),\n",
    "    StructField(\"Gender\",StringType(),True),\n",
    "    StructField(\"PersonOccupation\",StringType(),True),\n",
    "    StructField(\"PersonAddress\",StringType(),True),\n",
    "    StructField(\"PersonOriginCity\",StringType(),True),\n",
    "    StructField(\"FlightNumber\",StringType(),True),\n",
    "    StructField(\"VisaType\",StringType(),True),\n",
    "    StructField(\"VisaIssuePlace\",StringType(),True)])\n",
    "\n",
    "Dim_Date_Schema = StructType([\n",
    "    StructField(\"DateTime\",DateType(),True),\n",
    "    StructField(\"Year\",IntegerType(),True),\n",
    "    StructField(\"Month\",IntegerType(),True),\n",
    "    StructField(\"Day\",IntegerType(),True)])\n",
    "\n",
    "Dim_PortTemperature_Schema = StructType([\n",
    "    StructField(\"PortCode\",StringType(),True),\n",
    "    StructField(\"City\",StringType(),True),\n",
    "    StructField(\"State\",StringType(),True),\n",
    "    StructField(\"Country\",StringType(),True),\n",
    "    StructField(\"AverageTemperature\",FloatType(),True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Conver the arrdate and depdate from SAS format to date format\n",
    "# Translate the i94mode to readable format\n",
    "Fact_immigration=Fact_immigration=spark.sql(\"\"\"\n",
    "SELECT distinct \n",
    "    cicid,\n",
    "    date_add('1960-01-01', arrdate),\n",
    "    date_add('1960-01-01', depdate),\n",
    "    CASE \n",
    "    WHEN i94visa=1 THEN \"Air\"\n",
    "    WHEN i94visa=2 THEN \"Sea\"\n",
    "    WHEN i94visa=3 THEN \"Land\"\n",
    "    ELSE \"Not reported\" END as i94mode,\n",
    "    entdepa,\n",
    "    entdepd,\n",
    "    entdepd,\n",
    "    i94mode,\n",
    "    i94port,\n",
    "    i94mon,\n",
    "    i94yr\n",
    "FROM df_spark \n",
    "where i94mode is not null\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Translate the i94visa type to readable format\n",
    "Dim_Admission=Dim_Admission=spark.sql(\"\"\"\n",
    "SELECT distinct \n",
    "    cicid,\n",
    "    admnum,\n",
    "    biryear,\n",
    "    gender,\n",
    "    occup,\n",
    "    i94addr,\n",
    "    i94cit,\n",
    "    fltno,\n",
    "CASE \n",
    "    WHEN i94visa=1 THEN \"Business\"\n",
    "    WHEN i94visa=2 THEN \"Pleasure\"\n",
    "    WHEN i94visa=3 THEN \"Student\"\n",
    "    ELSE \"Other\" END as i94visa,\n",
    "    visapost\n",
    "\n",
    "FROM df_spark where i94mode is not null\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "Dim_Date=Dim_Date=spark.sql(\"\"\"\n",
    "(SELECT distinct \n",
    "    date_add('1960-01-01', arrdate),\n",
    "    Year(date_add('1960-01-01', arrdate)),\n",
    "    Month(date_add('1960-01-01', arrdate)),\n",
    "    Day(date_add('1960-01-01', arrdate))\n",
    "FROM df_spark\n",
    "where arrdate is not null)\n",
    "UNION\n",
    "(SELECT distinct \n",
    "    date_add('1960-01-01', depdate),\n",
    "    Year(date_add('1960-01-01', depdate)),\n",
    "    Month(date_add('1960-01-01', depdate)),\n",
    "    Day(date_add('1960-01-01', depdate))\n",
    "FROM df_spark\n",
    "where arrdate is not null)\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "Dim_PortTemperature=spark.sql(\"\"\"\n",
    "SELECT distinct \n",
    "df_City.PortCode,\n",
    "upper(df_temp_tb.City), \n",
    "df_City.States,\n",
    "df_temp_tb.Country,\n",
    "avg(AverageTemperature) as AverageTemperature\n",
    "\n",
    "FROM df_temp_tb \n",
    "\n",
    "join df_City\n",
    "on upper(df_temp_tb.City)=upper(df_City.City)\n",
    "\n",
    "where df_temp_tb.Country='United States' and AverageTemperature is not null\n",
    "Group by \n",
    "df_City.PortCode,\n",
    "df_temp_tb.City, \n",
    "df_City.States,\n",
    "df_temp_tb.Country\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def readDataUsingSchema(df,schema):\n",
    "    df=spark.createDataFrame(df.rdd, schema=schema)\n",
    "    return df\n",
    "    \n",
    "Fact_immigration=readDataUsingSchema(Fact_immigration,Fact_immigration_Schema)\n",
    "Dim_Admission=readDataUsingSchema(Dim_Admission,Dim_Admission_Schema)\n",
    "Dim_Date=readDataUsingSchema(Dim_Date,Dim_Date_Schema)\n",
    "Dim_PortTemperature=readDataUsingSchema(Dim_PortTemperature,Dim_PortTemperature_Schema)\n",
    "\n",
    "#Fact_immigration = spark.createDataFrame(Fact_immigration.rdd, schema=Fact_immigration_Schema)\n",
    "#Dim_Admission = spark.createDataFrame(Dim_Admission.rdd, schema=Dim_Admission_Schema)\n",
    "#Dim_Date = spark.createDataFrame(Dim_Date.rdd, schema=Dim_Date_Schema)\n",
    "#Dim_PortTemperature = spark.createDataFrame(Dim_PortTemperature.rdd, schema=Dim_PortTemperature_Schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Write data to Parquest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def loadToParquest(df,outpath):\n",
    "    try:\n",
    "        df.write.mode(\"overwrite\").parquet(\"data/\"+outpath)\n",
    "    except:\n",
    "        raise ValueError(\"Error exporting the data\")\n",
    "    print(\" dataframe is loaded to\" +outpath)\n",
    "\n",
    "loadToParquest(Fact_immigration,\"Fact_immigration\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "loadToParquest(Dim_Date,\"Dim_Date\") \n",
    "loadToParquest(Dim_Admission,\"Dim_Admission\")\n",
    "loadToParquest(Dim_PortTemperature,\"Dim_PortTemperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.2.1 Data Quality Checks - Unit Tests for the script to ensure they are doing the right thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def readFromParquet(table):\n",
    "    df=spark.read.parquet(\"data/\"+table)\n",
    "    ##df.createOrReplaceTempView(table)\n",
    "    return df\n",
    "\n",
    "Fact_immigration=readFromParquet(\"Fact_immigration\")\n",
    "Dim_Date=readFromParquet(\"Dim_Date\")\n",
    "Dim_Admission=readFromParquet(\"Dim_Admission\")\n",
    "Dim_PortTemperature=readFromParquet(\"Dim_PortTemperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.2.1 Integrity constraints quality check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Fact_immigration passed.\n",
      "Table Dim_Admission passed.\n",
      "Table Dim_Date passed.\n",
      "Table Dim_PortTemperature passed.\n"
     ]
    }
   ],
   "source": [
    "# Integrity constraints quality check\n",
    "#Expected result: duplicate primary key does not exist \n",
    "\n",
    "def IntegrityConstraintsCheck(table,PK):\n",
    "    result=spark.sql(f\"\"\"\n",
    "    SELECT {PK} , count( {PK} )\n",
    "       from {table}\n",
    "       group by  {PK} \n",
    "       having  count( {PK} )>1\n",
    "        \"\"\") \n",
    "    if result.count() == 0:\n",
    "        print(f\"Table {table} passed.\")\n",
    "    else:\n",
    "        raise ValueError(\"Integrity ConstraintsCheck failed! Duplicated primary Key exist\")\n",
    "            \n",
    "        \n",
    "IntegrityConstraintsCheck( \"Fact_immigration\", \"CICID\")\n",
    "IntegrityConstraintsCheck( \"Dim_Admission\", \"CICID\")\n",
    "IntegrityConstraintsCheck( \"Dim_Date\", \"DateTime\")\n",
    "IntegrityConstraintsCheck( \"Dim_PortTemperature\", \"PortCode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.2.2 Data Quality Checks No empty table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: Fact_immigration has total 3096074 records.\n",
      "Table: Dim_Date has total 236 records.\n",
      "Table: Dim_Admission has total 3096074 records.\n",
      "Table: Dim_PortTemperature has total 96 records.\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "\n",
    "def row_count(table):\n",
    "        try:\n",
    "            df_spark=spark.read.parquet(\"data/\"+table)\n",
    "            RowCount = df_spark.count()\n",
    "        except:\n",
    "            RowCount=0\n",
    "        \n",
    "        \n",
    "        if RowCount <= 0:\n",
    "            raise ValueError(\"No data in the table!\")\n",
    "        else:\n",
    "            print(\"Table: \" + table + \" has total \" + str(RowCount) +\" records.\") \n",
    "    \n",
    "row_count(\"Fact_immigration\") \n",
    "row_count(\"Dim_Date\") \n",
    "row_count(\"Dim_Admission\") \n",
    "row_count(\"Dim_PortTemperature\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "* See the Data dictionary in the project folder.\n",
    "* Below is the is the ecidence tha the ETL has processed the result into the final data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----+-------------+------------------+---------------------+\n",
      "|I94Port|         City|State|      Country|AverageTemperature|count(DISTINCT CICID)|\n",
      "+-------+-------------+-----+-------------+------------------+---------------------+\n",
      "|    NYC|     NEW YORK|   NY|United States|          9.523295|               485913|\n",
      "|    MIA|        MIAMI|   FL|United States|         23.068924|               343939|\n",
      "|    LOS|  LOS ANGELES|   CA|United States|         15.878038|               310162|\n",
      "|    SFR|SAN FRANCISCO|   CA|United States|         14.447988|               152583|\n",
      "|    ORL|      ORLANDO|   FL|United States|         22.302603|               149194|\n",
      "+-------+-------------+-----+-------------+------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Demo: Does visitor to United States like to travel to warmer or cooler cities? \n",
    "\n",
    "popular_cities=spark.sql(\"\"\"\n",
    "SELECT \n",
    "I94Port,\n",
    "City,\n",
    "State,\n",
    "Country,\n",
    "AverageTemperature,\n",
    "Count(distinct Fact_immigration.CICID)\n",
    "FROM Fact_immigration\n",
    "left join Dim_PortTemperature\n",
    "on Fact_immigration.I94Port= Dim_PortTemperature.PortCode\n",
    "\n",
    "Group by \n",
    "I94Port,\n",
    "City,\n",
    "State,\n",
    "Country,\n",
    "AverageTemperature\n",
    "\n",
    "Order by Count(distinct Fact_immigration.CICID) desc\n",
    "\"\"\")\n",
    "popular_cities.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+----------------+---------+-----------+-------------+----------+--------------+-------+--------+-------+\n",
      "|CICID|ArrivalDate|I94DepartureDate|i94modeID|ArrivalFlag|DepartureFlag|UpdateFlag|I94ArrivalMode|I94Port|I94Month|I94Year|\n",
      "+-----+-----------+----------------+---------+-----------+-------------+----------+--------------+-------+--------+-------+\n",
      "| 75.0| 2016-04-01|      2016-04-14|      Sea|          O|            I|         I|           1.0|    ATL|     4.0| 2016.0|\n",
      "|174.0| 2016-04-01|            null|      Sea|          G|         null|      null|           1.0|    WAS|     4.0| 2016.0|\n",
      "|267.0| 2016-04-01|      2016-04-15|      Sea|          G|            O|         O|           1.0|    NYC|     4.0| 2016.0|\n",
      "|609.0| 2016-04-01|      2016-08-21|     Land|          G|            O|         O|           1.0|    BOS|     4.0| 2016.0|\n",
      "|952.0| 2016-04-01|      2016-04-05|      Sea|          G|            O|         O|           1.0|    NEW|     4.0| 2016.0|\n",
      "+-----+-----------+----------------+---------+-----------+-------------+----------+--------------+-------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demo the Fact_immigration table \n",
    "Fact_immigration_Out=spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM Fact_immigration\n",
    "\"\"\")\n",
    "Fact_immigration_Out.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+\n",
      "|  DateTime|Year|Month|Day|\n",
      "+----------+----+-----+---+\n",
      "|2016-05-04|2016|    5|  4|\n",
      "|2016-05-26|2016|    5| 26|\n",
      "|2016-05-19|2016|    5| 19|\n",
      "|2016-08-06|2016|    8|  6|\n",
      "|2016-04-06|2016|    4|  6|\n",
      "+----------+----+-----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Demo the Dim_Date table\n",
    "Dim_Date_Out=spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM Dim_Date\n",
    "\"\"\")\n",
    "Dim_Date_Out.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+---------+------+----------------+-------------+----------------+------------+--------+--------------+\n",
      "| CICID|AddmissionNumber|BirthYear|Gender|PersonOccupation|PersonAddress|PersonOriginCity|FlightNumber|VisaType|VisaIssuePlace|\n",
      "+------+----------------+---------+------+----------------+-------------+----------------+------------+--------+--------------+\n",
      "| 267.0|    5.5457133E10|   1957.0|     F|            null|           NY|           103.0|       00404|Pleasure|          null|\n",
      "| 675.0|      5.54392E10|   1984.0|     F|            null|           CA|           103.0|       00066|Pleasure|          null|\n",
      "|1371.0|    5.5420494E10|   1986.0|     M|            null|           NY|           104.0|       01401|Pleasure|          null|\n",
      "|1548.0|     6.6757818E8|   1949.0|     F|            null|           NY|           104.0|           6|Pleasure|          null|\n",
      "|1766.0|    5.5422444E10|   1995.0|     F|            null|           OR|           104.0|       00179|Pleasure|          null|\n",
      "+------+----------------+---------+------+----------------+-------------+----------------+------------+--------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Demo the Dim_Admission table\n",
    "Dim_Admission_Out=spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM Dim_Admission\n",
    "\"\"\")\n",
    "Dim_Admission_Out.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+-----+-------------+------------------+\n",
      "|PortCode|            City|State|      Country|AverageTemperature|\n",
      "+--------+----------------+-----+-------------+------------------+\n",
      "|     COS|COLORADO SPRINGS|   CO|United States|          8.777836|\n",
      "|     SLC|  SALT LAKE CITY|   UT|United States|         10.177263|\n",
      "|     FAY|    FAYETTEVILLE|   NC|United States|         16.415527|\n",
      "|     PHI|    PHILADELPHIA|   PA|United States|         11.855868|\n",
      "|     CRP|  CORPUS CHRISTI|   TX|United States|         21.533556|\n",
      "+--------+----------------+-----+-------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Dim_Temperature_Out=spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM Dim_PortTemperature\n",
    "\"\"\")\n",
    "Dim_Temperature_Out.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Choice of tools and technologies for the project.\n",
    "* Used Pandas to explore and assess the sample data. As the sample file is small and pandas data format is easy to read. \n",
    "* Used PySpark to process the large files and move the data to parquet in data lake. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#####  Propose how often the data should be updated and why.\n",
    "\n",
    "I94 Immigration Data \n",
    "World Temperature Data\n",
    "* Depends on the frequence where source data is updated, the data lake can be updated accordeling. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Future Design Considerations\n",
    "* If the data was increased by 100x, I would process the data load in smaller batch. Also if buget allows, I would add more note to spark servers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " * If data populates a dashboard that must be updated on a daily basis by 7am every day,  I would use Airflow to schedule the load. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* If the database needed to be accessed by 100+ people, based on the users needs I will denomalized the data to so that users will not be to query multiple tables to reduce traffic. And if cost allow, move the data to AWS Redshipt database. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
